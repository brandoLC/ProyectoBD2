# BD2 Proyecto â€” Database Storage Engine

**Sistema completo de almacenamiento en disco con 4 Ã­ndices implementados y mÃ©tricas de I/O real**

## ğŸ¯ CaracterÃ­sticas Completadas

âœ… **4 Ãndices Implementados**

- **Sequential File** - BÃºsqueda secuencial con bloques
- **ISAM** - Ãndice multinivel (3 niveles: L1â†’L2â†’Buckets + Overflow)
- **Extendible Hash** - Hash dinÃ¡mico con directorio extensible
- **B+ Tree** - Ãrbol B+ balanceado con hojas enlazadas

âœ… **Operaciones CRUD Completas**

- `SELECT * FROM table` - Escaneo completo
- `SELECT * FROM table WHERE key = value` - BÃºsqueda por igualdad
- `SELECT * FROM table WHERE key BETWEEN a AND b` - BÃºsqueda por rango
- `INSERT INTO table (cols) VALUES (vals)` - InserciÃ³n de registros
- `DELETE FROM table WHERE key = value` - EliminaciÃ³n con persistencia en disco

âœ… **MÃ©tricas de I/O Real**

- Contador de reads/writes para todas las operaciones
- Persistencia en disco para ISAM, Ext Hash y B+ Tree
- Archivos separados para Ã­ndices (buckets, overflow, directorio)
- Sistema de buffer pool configurable

âœ… **Interfaz de Usuario Completa**

- Streamlit UI interactiva con 6 botones de consultas ejemplo
- Editor SQL con syntax highlighting
- VisualizaciÃ³n de resultados en tabla
- MÃ©tricas de performance (tiempo, I/O operations)
- FastAPI backend con Swagger docs

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

### OpciÃ³n 1: Local (Desarrollo)

```bash
# Crear entorno virtual
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt

# Inicializar base de datos (Kaggle dataset - 9.5K registros)
python scripts/load_all_9k.py

# Ejecutar Backend API (terminal 1)
uvicorn api.main:app --reload --port 8000

# Ejecutar UI (terminal 2)
streamlit run ui/app.py --server.port 8501
```

### OpciÃ³n 2: Docker Compose (Recomendado para ProducciÃ³n)

```bash
# Clonar repositorio
git clone https://github.com/brandoLC/ProyectoBD2.git
cd ProyectoBD2

# Levantar todo el sistema
docker compose up --build
```

**Primera ejecuciÃ³n (~5 minutos):**

- ğŸ—ï¸ Construye imÃ¡genes Docker
- ğŸ—„ï¸ Carga automÃ¡ticamente 9,551 registros en 4 tablas
- âœ… Inicia API + UI

**Ejecuciones subsecuentes (~15 segundos):**

- âš¡ Usa datos ya cargados
- âš¡ Inicia directo

**Endpoints:**

- ğŸ¨ UI Streamlit: http://localhost:8501
- ğŸ“¡ API FastAPI: http://localhost:8000/docs
- â¤ï¸ Health Check: http://localhost:8000/health

**MÃ¡s detalles:** Ver [`DOCKER_GUIDE.md`](DOCKER_GUIDE.md)

## ğŸ“Š Resultados de Benchmark (100 registros)

| Ãndice         | BUILD          | SELECT=          | RANGE(10)       | INSERT         | DELETE             |
| -------------- | -------------- | ---------------- | --------------- | -------------- | ------------------ |
| **Sequential** | 137ms (5W)     | 13.27ms (2R)     | **0.18ms (1R)** | 6.57ms (1R/2W) | 10.72ms (1R/2W)    |
| **ISAM**       | 157ms (5W)     | **11.93ms (5R)** | 10.92ms (5R)    | 6.85ms (5R/1W) | **0.26ms (5R/1W)** |
| **Ext Hash**   | **128ms (4W)** | 45.27ms (3R)     | N/A             | 1.07ms (2R/2W) | 0.28ms (4R/4W)     |
| **B+ Tree**    | 153ms (5W)     | 14.90ms (4R)     | 5.11ms (4R)     | 7.13ms (4R/2W) | 0.27ms (4R/4W)     |

**Conclusiones:**

- **Ext Hash**: MÃ¡s rÃ¡pido en BUILD (menos writes), pero SELECT= es lento
- **ISAM**: Excelente para SELECT= y DELETE (acceso directo por Ã­ndice)
- **Sequential**: Insuperable en RANGE pequeÃ±os (acceso contiguo en disco)
- **B+ Tree**: Balanceado en todas las operaciones, mejor para RANGE grandes

## ğŸ“– Uso del Sistema

### 1. Cargar Datos desde CSV

```bash
# Cargar dataset completo de Kaggle (9,551 registros) en las 4 tablas
python scripts/load_all_9k.py

# Limpiar storage si necesitas empezar de cero
python scripts/clean_storage.py
```

### 2. Ejecutar Consultas SQL

#### En la UI (Streamlit):

- BotÃ³n **"ğŸ” Buscar"** â†’ `SELECT * FROM restaurants_isam WHERE "Restaurant ID" = 6300002`
- BotÃ³n **"ğŸ“Š Rango"** â†’ `SELECT * FROM restaurants_isam WHERE "Restaurant ID" BETWEEN 6300000 AND 6300100`
- BotÃ³n **"â• Insertar"** â†’ Inserta nuevo registro
- BotÃ³n **"âŒ Eliminar"** â†’ `DELETE FROM restaurants WHERE "Restaurant ID" = 99999999`

#### Desde Python:

```python
from sql.parser import parse
from sql.executor import Executor

executor = Executor()

# SELECT con igualdad
result = executor.execute(parse("SELECT * FROM restaurants_hash WHERE \"Restaurant ID\" = 6300002"))
print(f"Tiempo: {result['execution_time_ms']:.2f}ms")
print(f"I/O: {result['io']['disk_reads']}R/{result['io']['disk_writes']}W")

# DELETE
result = executor.execute(parse("DELETE FROM restaurants_isam WHERE \"Restaurant ID\" = 6300002"))
print(f"Eliminados: {result['deleted']} registros")
```

### 3. Benchmark de ComparaciÃ³n

```bash
# Ejecutar benchmark completo con 9.5K registros
python scripts/benchmark_9k.py

# Generar grÃ¡ficos de visualizaciÃ³n
python scripts/visualize_benchmark.py
```

Genera tabla comparativa y CSV con resultados (guardados en `results/`):

```
Benchmark Results (100 records)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Operation    Sequential    ISAM           Ext Hash       B+ Tree
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BUILD        137ms (5W)    157ms (5W)     128ms (4W)     153ms (5W)
SELECT=      13ms (2R)     12ms (5R)      45ms (3R)      15ms (4R)
...
```

### 4. Limpiar Storage

```bash
python scripts/clean_storage.py
```

Elimina todos los archivos `.dat`, `.idx` y reinicia `catalog.json`.

## ğŸ—ï¸ Arquitectura del Sistema

### Estructura de Directorios

```
Proyecto/
â”œâ”€â”€ core/                   # Core engine
â”‚   â”œâ”€â”€ disk_manager.py      # GestiÃ³n de pÃ¡ginas en disco
â”‚   â”œâ”€â”€ buffer_pool.py       # Buffer pool con LRU
â”‚   â”œâ”€â”€ io_metrics.py        # Contador de I/O
â”‚   â”œâ”€â”€ table.py             # AbstracciÃ³n de tabla
â”‚   â””â”€â”€ schema.py            # DefiniciÃ³n de esquema
â”œâ”€â”€ indexes/                # Ãndices implementados
â”‚   â”œâ”€â”€ sequential.py        # Sequential File con bloques
â”‚   â”œâ”€â”€ isam.py             # ISAM 3-level + overflow
â”‚   â”œâ”€â”€ ext_hash.py         # Extendible Hash dinÃ¡mico
â”‚   â””â”€â”€ bplustree.py        # B+ Tree balanceado
â”œâ”€â”€ sql/                    # Motor SQL
â”‚   â”œâ”€â”€ parser.py           # Parser SQL â†’ AST
â”‚   â”œâ”€â”€ planner.py          # Query planner
â”‚   â””â”€â”€ executor.py         # Executor con mÃ©tricas
â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â””â”€â”€ main.py             # Endpoints REST
â”œâ”€â”€ ui/                     # Streamlit frontend
â”‚   â””â”€â”€ app.py              # Interfaz interactiva
â”œâ”€â”€ scripts/                # Scripts de utilidad
â”‚   â”œâ”€â”€ load_all_9k.py      # Cargar dataset completo
â”‚   â”œâ”€â”€ benchmark_9k.py     # Benchmark con 9.5K registros
â”‚   â”œâ”€â”€ visualize_benchmark.py  # Generar grÃ¡ficos
â”‚   â””â”€â”€ clean_storage.py    # Limpiar storage
â”œâ”€â”€ results/                # Resultados de benchmarks
â”‚   â”œâ”€â”€ benchmark_9k_results_*.csv  # Resultados en CSV
â”‚   â””â”€â”€ *.png               # GrÃ¡ficos de comparaciÃ³n
â”œâ”€â”€ tests/                  # Tests unitarios
â”‚   â”œâ”€â”€ test_indexes_basic.py   # Tests bÃ¡sicos de Ã­ndices
â”‚   â”œâ”€â”€ test_delete_real_io.py  # Tests de DELETE con I/O
â”‚   â””â”€â”€ test_delete_sql.py      # Tests de DELETE SQL
â”œâ”€â”€ docs/                   # DocumentaciÃ³n adicional
â”‚   â”œâ”€â”€ GUIA_USO.md         # GuÃ­a de uso detallada
â”‚   â”œâ”€â”€ QUERIES_KAGGLE.md   # Ejemplos de queries
â”‚   â””â”€â”€ TESTING_GUIDE.md    # GuÃ­a de testing
â”œâ”€â”€ storage/                # Archivos de datos persistentes
â”‚   â”œâ”€â”€ *.dat               # Archivos de datos de tablas
â”‚   â”œâ”€â”€ *_buckets.dat       # Buckets de ISAM/Hash
â”‚   â”œâ”€â”€ *_l1.idx            # Ãndice L1 de ISAM
â”‚   â”œâ”€â”€ *_l2.idx            # Ãndice L2 de ISAM
â”‚   â””â”€â”€ catalog.json        # CatÃ¡logo de tablas
â””â”€â”€ data/                   # Datasets CSV
    â””â”€â”€ kaggle_Dataset .csv # Dataset completo (9.5K registros)
```

### Flujo de una Consulta

```
Usuario (SQL) â†’ Parser â†’ AST â†’ Planner â†’ Executor
                                            â†“
                                      Table.search()
                                            â†“
                                    Index.search(key)
                                            â†“
                                    DiskManager.read()
                                            â†“
                                    BufferPool (cache)
                                            â†“
                                    IOMetrics.count()
                                            â†“
                                    Resultado + MÃ©tricas
```

## ğŸ”§ Detalles de ImplementaciÃ³n

### ISAM (3 niveles)

- **L1**: Ãndice principal (primeras claves de L2)
- **L2**: Ãndice secundario (primeras claves de buckets)
- **Buckets**: Datos ordenados por clave
- **Overflow**: Registros insertados despuÃ©s del build
- **Archivos**: `table_isam_l1.idx`, `table_isam_l2.idx`, `table_isam_buckets.dat`, `table_isam_overflow.dat`

### Extendible Hash

- **Directorio**: Array de bucket_ids con profundidad global
- **Buckets**: Registros hash con profundidad local
- **Splits**: DuplicaciÃ³n de directorio cuando bucket lleno
- **Hash**: `hash(key) % (2 ** depth)`
- **Archivos**: `table_extendiblehash`, `table_exthash_buckets.dat`, `table_exthash_overflow.dat`

### B+ Tree

- **Nodos internos**: Solo claves (order=4)
- **Hojas**: Claves + datos, enlazadas para range queries
- **Balanceo**: Split al insertar si lleno
- **Archivos**: `table_bplustree`, `table_bplustree_leaves.dat`, `table_bplustree_overflow.dat`

### Sequential

- **Bloques**: Registros agrupados en bloques de tamaÃ±o fijo
- **BÃºsqueda**: Escaneo secuencial de bloques
- **Overflow**: Registros insertados al final
- **Archivos**: `table_sequential`, `table_sequential_blocks.dat`, `table_sequential_overflow.dat`

## ğŸ§ª Testing

```bash
# Test DELETE en los 4 Ã­ndices
python tests/test_delete_sql.py

# Test de I/O real con DELETE
python tests/test_delete_real_io.py

# Tests bÃ¡sicos de Ã­ndices
pytest tests/test_indexes_basic.py -v
```

## ğŸ“ˆ Benchmark con Dataset Completo

```bash
# Cargar datos (9,551 registros)
python scripts/load_all_9k.py

# Ejecutar benchmark
python scripts/benchmark_9k.py

# Generar grÃ¡ficos
python scripts/visualize_benchmark.py
```

Ver resultados en la carpeta `results/`
python init_kaggle_db.py # Modificar para usar kaggle_Dataset.csv
python benchmark_comparison.py

````

## ğŸ› Troubleshooting

### Error: "No such file or directory"

```bash
python clean_storage.py
python init_kaggle_db.py
````

### Error: "Key not found"

- Verificar que el registro existe: `SELECT * FROM table`
- Revisar formato de clave (string vs int)

### Performance lento

- Revisar dataset size con `wc -l data/kaggle_Dataset_*.csv`
- Aumentar buffer pool size en `core/buffer_pool.py`
- Usar Ã­ndice apropiado (ISAM para igualdad, Sequential para ranges)

## ğŸ“ PrÃ³ximas Mejoras

- [ ] R-Tree para bÃºsquedas espaciales
- [ ] Soporte para mÃºltiples Ã­ndices por tabla
- [ ] Transactions y ACID properties
- [ ] Query optimizer con estadÃ­sticas
- [ ] VisualizaciÃ³n de estructura de Ã­ndices
- [ ] GrÃ¡ficos de performance con matplotlib
- [ ] Tests de concurrencia

## ğŸ‘¥ Autores

Universidad - Bases de Datos 2 (2025-2)

---

**Estado del Proyecto**: âœ… **COMPLETO** - 4 Ã­ndices implementados, CRUD funcional, benchmarks validados
